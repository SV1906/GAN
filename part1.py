# -*- coding: utf-8 -*-
"""part1_GAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NmdExVnJle_-cyHf-uCtj2EEIF-2SaEK

# CS 5814 Homework 4, Part 1: Generative Adversarial Networks
"""

from google.colab import files
import zipfile
import os
# Create a zip file containing the folder you want to upload
folder_to_upload = "/path/to/your/folder"
zip_file_name = "folder_to_upload.zip"
with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zipf:
    for root, dirs, files in os.walk(folder_to_upload):
        for file in files:
            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), folder_to_upload))
# Upload the zip file
uploaded = files.upload()

import zipfile
import os

# Specify the path to the zip file you uploaded
zip_file_path = "/content/cats.zip"

# Specify the directory where you want to extract and save the contents
extracted_folder_path = "/content/cats"

# Create a directory to extract the contents if it doesn't exist
if not os.path.exists(extracted_folder_path):
    os.makedirs(extracted_folder_path)

# Extract and save the contents of the zip file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extracted_folder_path)

print("Files extracted and saved successfully to:", extracted_folder_path)

# Commented out IPython magic to ensure Python compatibility.
import torch
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.datasets import ImageFolder
import matplotlib.pyplot as plt
import numpy as np

# %matplotlib inline
plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

# %load_ext autoreload
# %autoreload 2



# %pip install torchvision

from google.colab import drive
drive.mount("/content/drive")

# import os
# cwd = os.getcwd()

# cwd

# import os
# datadir = "./part1/" # path to the homework
# if not os.path.exists(cwd):
#   !ln -s "" $datadir # path to the homework
# os.chdir(cwd)
# !pwd

import zipfile
import os

# Specify the path to the zip file you uploaded
zip_file_path = "/content/cats.zip"

# Specify the directory where you want to extract and save the contents
extracted_folder_path = "/content/cats"

# Create a directory to extract the contents if it doesn't exist
if not os.path.exists(extracted_folder_path):
    os.makedirs(extracted_folder_path)

# Extract and save the contents of the zip file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extracted_folder_path)

print("Files extracted and saved successfully to:", extracted_folder_path)

# Commented out IPython magic to ensure Python compatibility.
import torch
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.datasets import ImageFolder
import matplotlib.pyplot as plt
import numpy as np

# %matplotlib inline
plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

# %load_ext autoreload
# %autoreload 2

uploaded = files.upload()

from train import train

device = torch.device("cuda:0" if torch.cuda.is_available() else "gpu")

device

"""# GAN loss functions

You'll need to implement two different loss functions. The first is the loss from the [original GAN paper](https://arxiv.org/pdf/1406.2661.pdf). The next is the loss from [LS-GAN](https://arxiv.org/abs/1611.04076).

### GAN loss

**TODO:** You need to implement the `discriminator_loss` and `generator_loss` functions in `gan/losses.py`.

The generator loss is given by:
$$\ell_G  =  -\mathbb{E}_{z \sim p(z)}\left[\log D(G(z))\right]$$
and the discriminator loss is:
$$ \ell_D = -\mathbb{E}_{x \sim p_\text{data}}\left[\log D(x)\right] - \mathbb{E}_{z \sim p(z)}\left[\log \left(1-D(G(z))\right)\right]$$

Note that these equations are negated because we will be *minimizing* these losses.

**HINTS**: Use the `torch.nn.functional.binary_cross_entropy_with_logits` function to compute the binary cross entropy loss since it is more numerically stable than using a softmax followed by BCE loss.

We will be averaging over the elements of the minibatch instead of computing the expectation of $\log D(G(z))$, $\log D(x)$ and $\log \left(1-D(G(z))\right)$.
"""

from losses import discriminator_loss, generator_loss

"""### Least Squares GAN loss

**TODO:** You need to implement the `ls_discriminator_loss` and `ls_generator_loss` functions in `gan/losses.py`.

[Least Squares GAN](https://arxiv.org/abs/1611.04076) is an alernative to the original GAN loss function.
For this part, all you need to do change the loss function and retrain the model.

Specifically, you'll implement equation (9) in the paper:
$$\ell_G  =  \frac{1}{2}\mathbb{E}_{z \sim p(z)}\left[\left(D(G(z))-1\right)^2\right]$$
and the discriminator loss:
$$ \ell_D = \frac{1}{2}\mathbb{E}_{x \sim p_\text{data}}\left[\left(D(x)-1\right)^2\right] + \frac{1}{2}\mathbb{E}_{z \sim p(z)}\left[ \left(D(G(z))\right)^2\right]$$
"""

from losses import ls_discriminator_loss, ls_generator_loss

"""# GAN model architecture

**TODO:** Next, you'll need to implement the `Discriminator` and `Generator` networks in `gan/models.py`.

We'll be using the architecture from [DCGAN](https://arxiv.org/pdf/1511.06434.pdf):

**Discriminator:**

- convolutional layer with in_channels=3, out_channels=128, kernel=4, stride=2
- convolutional layer with in_channels=128, out_channels=256, kernel=4, stride=2
- batch norm
- convolutional layer with in_channels=256, out_channels=512, kernel=4, stride=2
- batch norm
- convolutional layer with in_channels=512, out_channels=1024, kernel=4, stride=2
- batch norm
- convolutional layer with in_channels=1024, out_channels=1, kernel=4, stride=1

Use padding = 1 (not 0) for all the convolutional layers.

You can either use LeakyReLu throughout the discriminator (and a negative slope value of 0.2) or just use relu.

The discriminator will output a single score for each sample.The output of your discriminator should be a single value score corresponding to each input sample.


**Generator:**

**Note:** Here, you'll need to use transposed convolution (sometimes known as fractionally-strided convolution). This function is implemented in pytorch as `torch.nn.ConvTranspose2d`.

- transpose convolution with in_channels=NOISE_DIM, out_channels=1024, kernel=4, stride=1
- batch norm
- transpose convolution with in_channels=1024, out_channels=512, kernel=4, stride=2
- batch norm
- transpose convolution with in_channels=512, out_channels=256, kernel=4, stride=2
- batch norm
- transpose convolution with in_channels=256, out_channels=128, kernel=4, stride=2
- batch norm
- transpose convolution with in_channels=128, out_channels=3, kernel=4, stride=2

The generator network will need to map the output values between -1 and 1 using a `tanh` nonlinearity. The output should be of size 64x64 with 3 channels for each sample (equal dimensions to the images from the dataset).
"""

from models import Discriminator, Generator

"""# Data loading

Our dataset is pretty small, so in order to prevent overfitting, we need to perform data augmentation.

**TODO:** You'll need to implement some data augmentation by adding new transforms to the cell below.

The easiest you can do is just use the RandomCrop and ColorJitter, but feel free to play around with other augmentations as well to see how it affects the performance of your model. See https://pytorch.org/vision/stable/transforms.html.
"""

batch_size = 32
imsize = 64
cat_root = '/content/cats'

# Define your transformations
transform_augment = transforms.Compose([
    transforms.Resize((imsize, imsize)), # Ensures all images are the same size
    transforms.RandomCrop(imsize, padding=4),  # Pads and then crops
    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(15),
    transforms.ToTensor(),  # Converts to tensor
])

# Create your ImageFolder dataset
cat_train = ImageFolder(root=cat_root, transform=transform_augment)

# Create DataLoader
cat_loader_train = DataLoader(cat_train, batch_size=batch_size, shuffle=True, drop_last=True)

"""### Visualize dataset"""

from gan.utils import show_images

for batch in cat_loader_train:
    imgs = batch[0].numpy().squeeze()
    show_images(imgs, color=True)
    break

"""# Training


**TODO:** You need to write the training loop in `gan/train.py`.
"""

NOISE_DIM = 100
NUM_EPOCHS = 50
learning_rate = 0.001

"""### Train GAN"""

D = Discriminator().to(device)
G = Generator(noise_dim=NOISE_DIM).to(device)

D_optimizer = torch.optim.Adam(D.parameters(), lr=learning_rate, betas = (0.5, 0.999))
G_optimizer = torch.optim.Adam(G.parameters(), lr=learning_rate, betas = (0.5, 0.999))

# original gan
train(D, G, D_optimizer, G_optimizer, discriminator_loss,
          generator_loss, num_epochs=NUM_EPOCHS, show_every=250,
          batch_size=batch_size, train_loader=cat_loader_train, device=device)

"""### Train LS-GAN"""

D = Discriminator().to(device)
G = Generator(noise_dim=NOISE_DIM).to(device)

D_optimizer = torch.optim.Adam(D.parameters(), lr=learning_rate, betas = (0.5, 0.999))
G_optimizer = torch.optim.Adam(G.parameters(), lr=learning_rate, betas = (0.5, 0.999))

# ls-gan
train(D, G, D_optimizer, G_optimizer, ls_discriminator_loss,
          ls_generator_loss, num_epochs=NUM_EPOCHS, show_every=250,
          batch_size=batch_size, train_loader=cat_loader_train, device=device)

"""# WGAN Loss"""

from losses import wg_generator_loss, wg_discriminator_loss

D = Discriminator().to(device)
G = Generator(noise_dim=NOISE_DIM).to(device)

D_optimizer = torch.optim.Adam(D.parameters(), lr=learning_rate, betas = (0.5, 0.999))
G_optimizer = torch.optim.Adam(G.parameters(), lr=learning_rate, betas = (0.5, 0.999))

# ls-gan
train(D, G, D_optimizer, G_optimizer, wg_discriminator_loss,
          wg_generator_loss, num_epochs=NUM_EPOCHS, show_every=250,
          batch_size=batch_size, train_loader=cat_loader_train, device=device)